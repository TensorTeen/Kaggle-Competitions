{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-11T03:30:03.288344700Z",
     "start_time": "2023-08-11T03:29:26.830185100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-model-analysis"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  pip subprocess to install build dependencies did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [9 lines of output]\n",
      "  Ignoring numpy: markers 'python_version < \"3.8\"' don't match your environment\n",
      "  Ignoring numpy: markers 'python_version == \"3.8\"' don't match your environment\n",
      "  Ignoring numpy: markers 'python_version == \"3.9\"' don't match your environment\n",
      "  Collecting cython>=0.29\n",
      "    Obtaining dependency information for cython>=0.29 from https://files.pythonhosted.org/packages/7d/61/bf165c17a1296fd7db78e18fd8cbb157ab04060ec58d34ff319424af3e2d/Cython-3.0.0-cp311-cp311-win_amd64.whl.metadata\n",
      "    Using cached Cython-3.0.0-cp311-cp311-win_amd64.whl.metadata (3.2 kB)\n",
      "  ERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\n",
      "  ERROR: Could not find a version that satisfies the requirement numpy==1.21.3 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0rc1, 1.23.0rc2, 1.23.0rc3, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0rc1, 1.24.0rc2, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4, 1.25.0rc1, 1.25.0, 1.25.1, 1.25.2)\n",
      "  ERROR: No matching distribution found for numpy==1.21.3\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "pip subprocess to install build dependencies did not run successfully.\n",
      "exit code: 1\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading tensorflow_model_analysis-0.44.0-py3-none-any.whl (1.9 MB)\n",
      "     ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.1/1.9 MB 1.6 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 0.2/1.9 MB 3.0 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 0.6/1.9 MB 4.8 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 1.1/1.9 MB 6.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 1.7/1.9 MB 7.7 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 1.7/1.9 MB 6.5 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 1.8/1.9 MB 5.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.9/1.9 MB 5.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.9/1.9 MB 5.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: absl-py<2.0.0,>=0.9 in c:\\akshay\\tools\\python\\lib\\site-packages (from tensorflow-model-analysis) (1.4.0)\n",
      "Collecting apache-beam[gcp]<3,>=2.40 (from tensorflow-model-analysis)\n",
      "  Obtaining dependency information for apache-beam[gcp]<3,>=2.40 from https://files.pythonhosted.org/packages/a7/8f/461ca31efd64bcc4ea0baff6fd44b1cc9079c4fe3f2fefed28fc0f064d8f/apache_beam-2.49.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading apache_beam-2.49.0-cp311-cp311-win_amd64.whl.metadata (5.9 kB)\n",
      "Collecting ipython<8,>=7 (from tensorflow-model-analysis)\n",
      "  Downloading ipython-7.34.0-py3-none-any.whl (793 kB)\n",
      "     ---------------------------------------- 0.0/793.8 kB ? eta -:--:--\n",
      "     ---------------------- -------------- 481.3/793.8 kB 15.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- 793.8/793.8 kB 12.6 MB/s eta 0:00:00\n",
      "Collecting ipywidgets<8,>=7 (from tensorflow-model-analysis)\n",
      "  Obtaining dependency information for ipywidgets<8,>=7 from https://files.pythonhosted.org/packages/dc/0f/efa0531c38e7651d7b229e4746ed83ddd286d0ad80a316b175a4b14e8c1c/ipywidgets-7.8.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading ipywidgets-7.8.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting numpy~=1.22.0 (from tensorflow-model-analysis)\n",
      "  Downloading numpy-1.22.4.zip (11.5 MB)\n",
      "     ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/11.5 MB 16.6 MB/s eta 0:00:01\n",
      "     -- ------------------------------------- 0.8/11.5 MB 12.8 MB/s eta 0:00:01\n",
      "     --- ------------------------------------ 0.9/11.5 MB 7.0 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 2.1/11.5 MB 12.1 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 2.7/11.5 MB 12.1 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 3.1/11.5 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 3.6/11.5 MB 12.2 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 4.1/11.5 MB 11.9 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 4.7/11.5 MB 11.9 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 5.2/11.5 MB 11.9 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 5.6/11.5 MB 12.0 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 6.2/11.5 MB 12.0 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 6.8/11.5 MB 12.0 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 7.2/11.5 MB 11.8 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 7.8/11.5 MB 11.9 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 8.4/11.5 MB 11.9 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 9.0/11.5 MB 11.9 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 9.5/11.5 MB 11.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 9.9/11.5 MB 12.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 10.4/11.5 MB 11.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 10.9/11.5 MB 11.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  11.4/11.5 MB 12.3 MB/s eta 0:00:01\n",
      "     --------------------------------------  11.5/11.5 MB 12.3 MB/s eta 0:00:01\n",
      "     --------------------------------------- 11.5/11.5 MB 11.7 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting pandas<2,>=1.0 (from tensorflow-model-analysis)\n",
      "  Downloading pandas-1.5.3-cp311-cp311-win_amd64.whl (10.3 MB)\n",
      "     ---------------------------------------- 0.0/10.3 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.6/10.3 MB 12.2 MB/s eta 0:00:01\n",
      "     ---- ----------------------------------- 1.2/10.3 MB 12.4 MB/s eta 0:00:01\n",
      "     ------ --------------------------------- 1.8/10.3 MB 12.3 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 2.3/10.3 MB 12.4 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 2.8/10.3 MB 11.9 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 3.3/10.3 MB 11.7 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 3.8/10.3 MB 12.1 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 4.3/10.3 MB 11.9 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 4.8/10.3 MB 11.7 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 5.1/10.3 MB 11.7 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 5.1/10.3 MB 11.7 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 5.2/10.3 MB 10.3 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 5.8/10.3 MB 10.2 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 6.1/10.3 MB 10.1 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 6.4/10.3 MB 9.8 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 6.7/10.3 MB 9.5 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 7.0/10.3 MB 9.3 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 7.2/10.3 MB 9.1 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 7.5/10.3 MB 8.8 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 7.7/10.3 MB 8.8 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 7.9/10.3 MB 8.5 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 8.2/10.3 MB 8.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 8.5/10.3 MB 8.5 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 8.8/10.3 MB 8.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 9.0/10.3 MB 8.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 9.2/10.3 MB 8.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 9.6/10.3 MB 8.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 9.9/10.3 MB 8.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  10.2/10.3 MB 8.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  10.3/10.3 MB 7.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  10.3/10.3 MB 7.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 10.3/10.3 MB 7.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: protobuf<5,>=3.20.3 in c:\\akshay\\tools\\python\\lib\\site-packages (from tensorflow-model-analysis) (4.23.4)\n",
      "Collecting pyarrow<7,>=6 (from tensorflow-model-analysis)\n",
      "  Downloading pyarrow-6.0.1.tar.gz (770 kB)\n",
      "     ---------------------------------------- 0.0/770.7 kB ? eta -:--:--\n",
      "     ---------------- -------------------- 337.9/770.7 kB 10.6 MB/s eta 0:00:01\n",
      "     ------------------------------ ------ 634.9/770.7 kB 10.1 MB/s eta 0:00:01\n",
      "     -------------------------------------  768.0/770.7 kB 8.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- 770.7/770.7 kB 7.0 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'error'\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-model-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Akshay\\Tools\\Python\\Lib\\site-packages\\tensorflow\\python\\ops\\distributions\\distribution.py:259: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From C:\\Akshay\\Tools\\Python\\Lib\\site-packages\\tensorflow\\python\\ops\\distributions\\bernoulli.py:165: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T17:01:01.101279400Z",
     "start_time": "2023-08-10T17:00:54.544823Z"
    }
   },
   "id": "5d157bdb1ff4ad17"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_graphs(history, metric):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history['val_'+metric], '')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([metric, 'val_'+metric])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T17:01:01.513384400Z",
     "start_time": "2023-08-10T17:01:01.101279400Z"
    }
   },
   "id": "c982dcdb584459f1"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T17:01:02.330759800Z",
     "start_time": "2023-08-10T17:01:01.513384400Z"
    }
   },
   "id": "d212570a57053e90"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "0       MAJOR LIBERAL RAG RELUCTANTLY PUBLISHES Articl...\n1       AT&T, Time Warner and the Death of PrivacyAT&T...\n2       The World Sees a Diminished AmericaThe World S...\n3       The Bolton Threat to Trump’s Middle East Polic...\n4       Life at migrant centers in Germany: RT talks t...\n                              ...                        \n7883    Italy's ruling PD slides further in polls as e...\n7884    Trump-backed Navy expansion would boost costs ...\n7885    Canada and E.U. Sign Trade Deal, Bucking Resis...\n7886    With Koch Brothers Academy, Conservatives Sett...\n7887    More than 50 arrested for looting in Miami dur...\nName: all, Length: 17438, dtype: object"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('test.csv')[[\"id\", \"all\"]]\n",
    "len(df_test)\n",
    "x_df_test = df_test['all']\n",
    "x_df = df['all']\n",
    "x_df_temp = x_df._append(x_df_test)\n",
    "y_df = df['label']\n",
    "x_df_temp"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T17:01:03.021419800Z",
     "start_time": "2023-08-10T17:01:02.333765200Z"
    }
   },
   "id": "3f1ecce9d58c0408"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 70000\n",
    "encoder = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,)\n",
    "encoder.adapt(x_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T17:01:06.383810100Z",
     "start_time": "2023-08-10T17:01:03.019416200Z"
    }
   },
   "id": "db321f9cef07a0c1"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_df,y_df,random_state=0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T17:01:06.393430Z",
     "start_time": "2023-08-10T17:01:06.385814300Z"
    }
   },
   "id": "18c9fad780f209df"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"text_vectorization\" (type TextVectorization).\n\nWhen using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(None, None) with rank=2\n\nCall arguments received by layer \"text_vectorization\" (type TextVectorization):\n  • inputs=tf.Tensor(shape=(None, None), dtype=string)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSequential\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEmbedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_dim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mencoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_vocabulary\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_dim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Use masking to handle the variable sequence lengths\u001B[39;49;00m\n\u001B[0;32m      7\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmask_zero\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mBidirectional\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLSTM\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDense\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mactivation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrelu\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDense\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Akshay\\Tools\\Python\\Lib\\site-packages\\tensorflow\\python\\trackable\\base.py:204\u001B[0m, in \u001B[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    202\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_self_setattr_tracking \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m    203\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 204\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[43mmethod\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    205\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    206\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_self_setattr_tracking \u001B[38;5;241m=\u001B[39m previous_value  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Akshay\\Tools\\Python\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32mC:\\Akshay\\Tools\\Python\\Lib\\site-packages\\keras\\src\\layers\\preprocessing\\text_vectorization.py:588\u001B[0m, in \u001B[0;36mTextVectorization._preprocess\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m    586\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inputs\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;241m.\u001B[39mrank \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    587\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m inputs\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 588\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    589\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhen using `TextVectorization` to tokenize strings, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    590\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthe input rank must be 1 or the last shape dimension \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    591\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmust be 1. Received: inputs.shape=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00minputs\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    592\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwith rank=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00minputs\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;241m.\u001B[39mrank\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    593\u001B[0m         )\n\u001B[0;32m    594\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    595\u001B[0m         inputs \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39msqueeze(inputs, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: Exception encountered when calling layer \"text_vectorization\" (type TextVectorization).\n\nWhen using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(None, None) with rank=2\n\nCall arguments received by layer \"text_vectorization\" (type TextVectorization):\n  • inputs=tf.Tensor(shape=(None, None), dtype=string)"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T16:54:39.487002800Z",
     "start_time": "2023-08-10T16:54:38.248469400Z"
    }
   },
   "id": "f4fa3d603d3f69b8"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "text_model_input = tf.keras.layers.Input(dtype=tf.string, shape=(1,))\n",
    "text_model_catprocess2 = encoder(text_model_input)\n",
    "text_model_embedd = tf.keras.layers.Embedding(70000, embedding_dim, name = 'embedding')(text_model_catprocess2)\n",
    "text_model_bid = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(text_model_embedd)\n",
    "text_model_bid2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))(text_model_bid)\n",
    "text_model_dense2 = tf.keras.layers.Dense(64, activation = 'relu')(text_model_bid2)\n",
    "text_model_output = tf.keras.layers.Dense(1, activation = 'sigmoid')(text_model_dense2)\n",
    "model = tf.keras.Model(text_model_input, text_model_output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T17:01:07.102183100Z",
     "start_time": "2023-08-10T17:01:06.392371300Z"
    }
   },
   "id": "a080df457981d18"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T17:01:07.118505900Z",
     "start_time": "2023-08-10T17:01:07.104182500Z"
    }
   },
   "id": "79502316653d1807"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "train_dataset = pd.DataFrame()\n",
    "train_dataset['text'] = x_train\n",
    "train_dataset['label'] = y_train\n",
    "\n",
    "test_dataset = pd.DataFrame()\n",
    "test_dataset['text'] = x_test\n",
    "test_dataset['label'] = y_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T17:01:07.150335100Z",
     "start_time": "2023-08-10T17:01:07.120526Z"
    }
   },
   "id": "69f6fa12054cadc9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Akshay\\Tools\\Python\\Lib\\site-packages\\keras\\src\\backend.py:5820: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224/224 [==============================] - 1322s 6s/step - loss: 0.4795 - accuracy: 0.8622 - val_loss: 0.3934 - val_accuracy: 0.8656\n",
      "Epoch 2/15\n",
      "158/224 [====================>.........] - ETA: 4:41 - loss: 0.3986 - accuracy: 0.8623"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train, epochs=15,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    validation_steps=30)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-08-10T17:01:07.130722500Z"
    }
   },
   "id": "d85dde75078c2f41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "dc7e2868bb7a26c4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "test_loss, test_acc = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a1afb89c9b7210ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_df_test"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "96bc4108467260c2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_df_test)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "f2f7faeccd6d8282"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred_rounded = y_pred.round()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "76cb119856e5e038"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_out = pd.DataFrame()\n",
    "df_out['id'] = df_test['id']\n",
    "df_out[\"label\"] = y_pred_rounded.astype(int)\n",
    "df_out.to_csv('submission.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "832f4809b11e9bf5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_out.label.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "4f937d208641fd0e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-10T16:54:39.509524400Z"
    }
   },
   "id": "aacd760639977f3e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-10T16:54:39.510525300Z"
    }
   },
   "id": "5389352b00f54baa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
