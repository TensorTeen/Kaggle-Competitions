{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Importing Libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib.pylab import plt\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T01:26:33.844201Z",
     "start_time": "2023-08-11T01:26:26.645492600Z"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\varsi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Akshay\\Tools\\Python\\Lib\\site-packages\\tensorflow\\python\\ops\\distributions\\distribution.py:259: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From C:\\Akshay\\Tools\\Python\\Lib\\site-packages\\tensorflow\\python\\ops\\distributions\\bernoulli.py:165: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Configurations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "news_data = pd.read_csv('train.csv')[[\"all\", \"label\"]]"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T01:26:40.922858800Z",
     "start_time": "2023-08-11T01:26:39.853521500Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class_names = ['fake', 'real'] # 0: fake, 1: real"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T01:26:40.924865300Z",
     "start_time": "2023-08-11T01:26:40.922345200Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "news_data.head()"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T01:26:41.292223100Z",
     "start_time": "2023-08-11T01:26:41.251948500Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                 all  label\n0  MAJOR LIBERAL RAG RELUCTANTLY PUBLISHES Articl...      1\n1  AT&T, Time Warner and the Death of PrivacyAT&T...      1\n2  The World Sees a Diminished AmericaThe World S...      1\n3  The Bolton Threat to Trump’s Middle East Polic...      1\n4  Life at migrant centers in Germany: RT talks t...      1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>all</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>MAJOR LIBERAL RAG RELUCTANTLY PUBLISHES Articl...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AT&amp;T, Time Warner and the Death of PrivacyAT&amp;T...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The World Sees a Diminished AmericaThe World S...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The Bolton Threat to Trump’s Middle East Polic...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Life at migrant centers in Germany: RT talks t...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "news_data.info()"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T01:26:45.333291600Z",
     "start_time": "2023-08-11T01:26:45.299731200Z"
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9550 entries, 0 to 9549\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   all     9550 non-null   object\n",
      " 1   label   9550 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 149.3+ KB\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "news_data.isnull().sum()"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T01:26:46.880052800Z",
     "start_time": "2023-08-11T01:26:46.842689800Z"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "all      0\nlabel    0\ndtype: int64"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "news_data = news_data.fillna('')"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T01:26:47.719486500Z",
     "start_time": "2023-08-11T01:26:47.685618800Z"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    " news_data.isnull().sum()"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T01:26:48.275760400Z",
     "start_time": "2023-08-11T01:26:48.245826200Z"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "all      0\nlabel    0\ndtype: int64"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "news_data.nunique()"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T01:26:49.333008Z",
     "start_time": "2023-08-11T01:26:49.236203600Z"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "all      8934\nlabel       2\ndtype: int64"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "label_count = news_data.label.value_counts()\n",
    "sns.barplot(label_count.index, label_count)\n",
    "plt.title('Target Count', fontsize=14)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T01:26:51.078767700Z",
     "start_time": "2023-08-11T01:26:50.150253600Z"
    }
   },
   "execution_count": 10,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "barplot() takes from 0 to 1 positional arguments but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m label_count \u001B[38;5;241m=\u001B[39m news_data\u001B[38;5;241m.\u001B[39mlabel\u001B[38;5;241m.\u001B[39mvalue_counts()\n\u001B[1;32m----> 2\u001B[0m \u001B[43msns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbarplot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabel_count\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_count\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m plt\u001B[38;5;241m.\u001B[39mtitle(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTarget Count\u001B[39m\u001B[38;5;124m'\u001B[39m, fontsize\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m14\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: barplot() takes from 0 to 1 positional arguments but 2 were given"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Building Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stemming & Extracting text data using TF-IDF"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "port = PorterStemmer()\n",
    "\n",
    "def stemming(text):\n",
    "    stem_text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    stem_text = stem_text.lower()\n",
    "    stem_text = stem_text.split()\n",
    "    \n",
    "    stem_text = [port.stem(word) for word in stem_text if not word in stopwords.words('english')]\n",
    "    stem_text = ' '.join(stem_text)\n",
    "    \n",
    "    return stem_text"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T01:26:56.391841600Z",
     "start_time": "2023-08-11T01:26:56.383247800Z"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since there are too many text data, we extracted only important words using TF-IDF."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def refinement_tfidf(text): # TF-IDF\n",
    "    ex = text.split('.')\n",
    "    ex = pd.DataFrame(ex)[0].apply(stemming)\n",
    "    \n",
    "    if ex[0] == '':\n",
    "        return text\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    try:\n",
    "        vectorizer.fit(ex)\n",
    "        features = vectorizer.transform(ex)\n",
    "    except:\n",
    "        return text\n",
    "    \n",
    "    feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "    feature_count = features.toarray().mean(axis=0)\n",
    "    \n",
    "    refined_text = ' '.join(feature_names[feature_count > 0.05]) # setting the importance weights according to TF-IDF\n",
    "    \n",
    "    return refined_text"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T01:27:00.971951500Z",
     "start_time": "2023-08-11T01:27:00.934868800Z"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "t0 = time.time()\n",
    "news_data['refined_tfidf'] = news_data['all'].apply(refinement_tfidf)\n",
    "print(time.time() - t0) # It takes about 45 minutes..."
   ],
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-08-11T01:27:02.464996Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "news_data.head()"
   ],
   "metadata": {
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "labels = news_data['label'].values\n",
    "texts = (news_data['title'] + ' ' + news_data['refined_tfidf']).values"
   ],
   "metadata": {
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ],
   "metadata": {
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "idx = np.random.choice(range(len(texts)))\n",
    "print(texts[idx])\n",
    "# Tokenized\n",
    "print(tokenizer.tokenize(texts[idx]))\n",
    "# Token to Integral\n",
    "print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(texts[idx])))"
   ],
   "metadata": {
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "max_len = 0\n",
    "id_len = []\n",
    "for text in texts:\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "    if len(input_ids) > 200:\n",
    "        continue\n",
    "    id_len.append(len(input_ids))\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "print('Max sentence length: ', max_len)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pd.Series(id_len).hist(bins=25)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def tokenize_map(sentence, labs='None'):\n",
    "    global labels\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for text in sentence:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            text,\n",
    "                            add_special_tokens = True, # [CLS] & [SEP]\n",
    "                            truncation = 'longest_first', # Control truncation\n",
    "                            max_length = 100, # Max length about texts\n",
    "                            pad_to_max_length = True, # Pad and truncate about sentences\n",
    "                            return_attention_mask = True, # Attention masks\n",
    "                            return_tensors = 'pt') # Return to pytorch tensors\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    \n",
    "    if labs != 'None':\n",
    "        labels = torch.tensor(labels)\n",
    "        return input_ids, attention_masks, labels\n",
    "    else:\n",
    "        return input_ids, attention_masks"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "input_ids, attention_masks, labels = tokenize_map(texts, labels)\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "len(train_dataset.indices), len(val_dataset.indices)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size = 32\n",
    "num_workers = 4\n",
    "\n",
    "train_loader = DataLoader(\n",
    "                dataset=train_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=num_workers)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "                dataset=val_dataset,\n",
    "                batch_size=1,\n",
    "                shuffle=False)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Loading Deep Learning Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We used BERT for natural language processing based on deep learning model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-uncased',\n",
    "        num_labels=2,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False)\n",
    "model.to(device)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                              lr=6e-6,\n",
    "                              eps=1e-8)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "num_epochs = 10\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "model.train()\n",
    "\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i, (ids, masks, labels) in enumerate(train_loader):\n",
    "        ids = ids.to(device)\n",
    "        masks = masks.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        loss = model(ids, token_type_ids=None, attention_mask=masks, labels=labels)[0]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print('{} / {}'.format(batch_size*(i+1), train_dataset.__len__()))\n",
    "            \n",
    "    print('Epoch: {}, Loss: {:.4f}'.format(epoch+1, total_loss / total_step))\n",
    "            \n",
    "torch.save(model.state_dict(), 'nets/BERT.ckpt')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "model.load_state_dict(torch.load('nets/BERT.ckpt', map_location=device))"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "score = 0\n",
    "for i, (ids, masks, labels) in enumerate(val_loader):\n",
    "    ids = ids.to(device)\n",
    "    masks = masks.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    g_labels = model(ids, token_type_ids=None, attention_mask=masks, labels=labels)[1]\n",
    "    \n",
    "    pred = torch.max(g_labels, 1)[1][0].item()\n",
    "    gt = labels[0].item()\n",
    "    \n",
    "    score += int(pred == gt)\n",
    "\n",
    "avg = score / len(val_dataset)\n",
    "print('Accuracy: {:.4f}\\n'.format(avg))"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "It showed 96.15% performance!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Run Demo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "for i in range(5):\n",
    "    idx = np.random.choice(range(len(news_data)))\n",
    "\n",
    "    print('\\033[1m'+news_data['title'][idx]+'\\n')\n",
    "    print('\\033[0m'+news_data['text'][idx])\n",
    "\n",
    "    label = news_data['label'][idx]\n",
    "    text = news_data['title'][idx] + ' ' + news_data['refined_tfidf'][idx]\n",
    "\n",
    "    input_id, attention_mask = tokenize_map([text])\n",
    "\n",
    "    g_label = model(input_id.to(device), token_type_ids=None, attention_mask=attention_mask.to(device))[0]\n",
    "    pred = torch.max(g_label, 1)[1][0].item()\n",
    "\n",
    "    print('\\n')\n",
    "    print('Predict: {}'.format(class_names[pred]))\n",
    "    print('GT: {}'.format(class_names[label]))\n",
    "    print('---------------------------------------------------------------------------------------------\\n')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
